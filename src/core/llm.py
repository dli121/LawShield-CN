import os
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶é‡Œçš„é…ç½®
load_dotenv()


class LLMClient:
    def __init__(self):
        self.client = OpenAI(
            api_key=os.getenv("LLM_API_KEY"), base_url=os.getenv("LLM_BASE_URL")
        )
        self.model = os.getenv("LLM_MODEL", "gpt-4o")
        print(f"ğŸ”Œ [LLMClient] å·²è¿æ¥è‡³: {self.model}")

    def chat(self, user_query: str) -> str:
        """
        å‘é€è¯·æ±‚ç»™å¤§æ¨¡å‹ï¼Œå¹¶è·å–å›å¤
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    # System Prompt: è®¾å®š AI çš„äººè®¾ï¼Œè¿™åœ¨æ²»ç†ä¸­ä¹Ÿå¾ˆé‡è¦
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹åˆè§„åŠ©æ‰‹ã€‚å›ç­”éœ€ä¸¥è°¨ã€å®¢è§‚ã€‚",
                    },
                    {"role": "user", "content": user_query},
                ],
                temperature=0.3,  # æ¸©åº¦ä½ä¸€ç‚¹ï¼Œå›ç­”æ›´ä¸¥è°¨
                max_completion_tokens=500,
            )

            return response.choices[0].message.content
        except Exception as e:
            print(f"âŒ [LLM Error] è°ƒç”¨å¤±è´¥: {e}")
            return "å¯¹ä¸èµ·ï¼ŒAI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"


# å•ä¾‹æ¨¡å¼
llm_client = LLMClient()
